Erm... i've kinda said what I was going to say here in the previous section

\subsection{Importance distributions}

The dimensionality of a fixed lag particle filter will generally be very large. Consider a problem where each target has a 4-dimensional kinematic state (postion and velocity in $x$ and $y$) and an observation-association index. This gives us five dimensions per target per time step. If we use a lag window with length $L=5$ and 5 targets we will have a 125 dimensional state. If we used a basic, bootstrap approach to such a problem, the chances of even a single particle following the correct path are negligible. Instead we must exploit the strong correlations between states arising from the structure of the problem. We first factorise the proposal thus:

%First we consider the importance distribution for a single target and a lag of $L$. The ``optimal'' distribution for this is

%\begin{multline}
%q(x_{t-L+1:t}, \lambda_{t-L+1:t}|x_{t-L}, Y_{t-L+1:t}) = P(x_{t-L+1:t}, \lambda_{t-L+1:t}|x_{t-L}, Y_{t-L+1:t}) = \\
%\frac{P(Y_{t-L+1:t}|x_{t-L+1:t}, \lambda_{t-L+1:t}) P(X_{t-L+1:t}, \lambda_{t-L+1:t}|x_{t-L})}{P(Y_{t-L+1:t}|x_{t-L})}
%\label{eq:}
%\end{multline}

%\begin{multline}
%q(x_{t-L+1:t}, \lambda_{t-L+1:t}|x_{t-L}, Y_{t-L+1:t}) = \\
%q(\lambda_t|x_{t-L}, Y_t) \prod_{k=t-L+1}^{t-1} {q(\lambda_k| \lambda_{k+1:t}, Y_{k:t}, x_{t-L})} \nonumber \\
%q(x_t|x_{t-L}, \lambda_{t-L+1:t}, Y_{t-L+1:t}) \prod_{k=t-L+1}^{t-1} {q(x_k|x_{t-L}, x_{k+1}, \lambda_{t-L+1:k}, Y_{t-L+1:k})} \nonumber
%\label{eq:}
%\end{multline}

\begin{IEEEeqnarray}{rCl}
q(x_{t-L+1:t}, \lambda_{t-L+1:t}|x_{t-L}, Y_{t-L+1:t}) & & \nonumber \\
 & = & q(\lambda_{t-L+1:t}|x_{t-L}, Y_{t-L+1:t}) \nonumber \\
 & = & q(x_{t-L+1:t}|x_{t-L}, \lambda_{t-L+1:t}, Y_{t-L+1:t})
\label{eq:}
\end{IEEEeqnarray}

%This can be sampled sequentially. We start with the last association variable, $\lambda_t$, then proceed backwards in time through $\lambda_{t-L+1:t-1}$. Once the association variables are set, the kinematic states may be sampled similarly using the method suggested in \cite{Doucet2006} and originally in \cite{Chib1996}, in which we first propose $x_t$ then sequentially backwards through $x_{t-L+1:t-1}$.

Thus we can sample first the association variables then the state variables. Each of these terms can be further factorised over time, as we shall see. If we were to replace each of the factors with its corresponding posterior distribution then this factorsiation would recreate the ``optimal'' importance distribution.% In general we will not be able to either sample from or calculate such a posterior. The exception is the case where we have linear-Gaussian dynamics.

\subsubsection{Association proposals}

The proposal for the association variables is a discrete distribution over the possible observations with which the target could be associated. Within the factorisation above, the ``optimal'' form of the proposal is:

\begin{IEEEeqnarray}{rCl}
q(\lambda_{t-L+1:t}|x_{t-L}, Y_{t-L+1:t}) & & \nonumber \\
 & \propto & P(Y_{t-L+1:t}|\lambda_{t-L+1:t},x_{t-L}) P(\lambda_{t-L+1:t}|x_{t-L}) \nonumber \\
 & = & \prod_{\substack{k=1:L\\tt=t-L+k}} \int P(Y_{tt}|x_{tt}, \lambda_k) P(x_{tt}|Y_{tt+1:t}, \lambda_{tt+1:t}, x_{t-L}) dx_{tt} P(\lambda_{tt})
\label{eq:GeneralAssocProp}
\end{IEEEeqnarray}

This can be calculated analytically for the linear-Gaussian case. For other cases, the EKF approximations may be used. The only problematic term in the expression above is the state distribution over $x_{tt}$, which will require $k$ integrals to calculate. Although this is will be analytic with Gaussian dynamics, its complexity may become unmanageable. This term represents the probability of states given the set of observations associated with this target at later times. We can render this calculation more manageable by replacing the whole set of future observations with just one.

\begin{equation}
P(x_k|Y_{k+1:t}, \lambda_{k+1:t}, x_{t-L}) \approx P(x_k|Y_{k+d}, \lambda_{k+d}, x_{t-L})
\label{eq:}
\end{equation}

In general we will use $d=1$, as the closest observation in time will give us the most information about $x_{tt}$. However, if the target is not detected at time $tt+1$, then we can increase $d$ to find the next detection of the target.

Using this approximation, for $\lambda_{t-L+k} \ne 0$, we have

\begin{equation}
q(\lambda_{t-L+1:t}|x_{t-L}, Y_{t-L+1:t}) \propto \prod_{\substack{k=1:L\\tt=t-L+k}} \mathcal{N}(y_{tt}^{(\lambda_{tt})}|m_{tt}, S_{tt})
\end{equation}

where usually

\begin{equation} S_{tt} = [ I - R^{-1} C_{tt} \Sigma_{tt} C_{tt} R^{-1} ]^{-1} \label{eq:} \end{equation}
\begin{equation} m_{tt} = S R^{-1} C_{tt} \Sigma_{tt} [ (A^d)^T C_{tt+d}^T R_d^{-1} y_{tt+d}^{\lambda_{tt+d}} + Q_k^{-1} A^k x_{t-L} ] \label{eq:} \end{equation}
\begin{equation} \Sigma_{tt} = [ C_{tt}^T R^{-1} C_{tt} + (A^d)^T C_{tt+d}^T R_d^{-1} C_{tt+d} A^d + Q_k^{-1}]^{-1} \label{eq:} \end{equation}
\begin{equation} Q_d = \sum_{l=0}^{d-1} {A^l Q (A^l)^T} \label{eq:} \end{equation}
\begin{equation} R_d = R + C_{tt+d} Q_d (C_{tt+d})^T \label{eq:} \end{equation}

We will need a different expression for the case when $tt=t$, because no future associations have yet been proposed. Similarly, if $tt<t$ but the future associations have all been proposed as missed detections, then there are no future observations to guide us, whatever choice of $d$ we use. In these cases we have:

\begin{equation} S_{tt} = R_d \end{equation}
\begin{equation} m_{tt} = C_{tt} A^k x_{t-L} \label{eq:} \end{equation}

For full derivations, see Appendix.

So we now have a full mechanism for association proposals.

Put in some pseudo-code here.

Add a bit about adapting the MD proposal when there is clearly no valid observation.


\subsubsection{Forward filtering backward sampling}

When the state space model is linear and both transition and observation densities are Gaussian, we can propose directly from the ``optimal'' importance distribution for the states. This is the forward-filtering-backward-sampling algorithm of \cite{Chib1999}. Starting with $x_t$:

\begin{equation}
q(x_t|x_{t-L}, \lambda_{t-L+1:t}, Y_{t-L+1:t}) = 
\label{eq:}
\end{equation}