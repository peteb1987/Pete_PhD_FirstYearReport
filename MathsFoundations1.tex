For the literature review, mathematical detail was kept to a minimum for clarity of presentation. In this chapter we revisit some basics of inference and particle filtering in order to establish notation and mathematical foundations for the later chapters.

Many tasks in signal processing, science in general, and indeed life, require us to make some estimate of an unknown quantity from indirect, incomplete, or inaccurate observations. By constructing a probabilistic model to explain how these observations depend on the underlying state, we can infer something about that state. Such an observation model may be expressed in terms of a likelihood function

\begin{equation}
P(Y|X)
\label{eq:LH}
\end{equation}

where $X$ is the state and $Y$ the observations. This is not the whole story - in many cases we are not estimating our unknown state `from scratch'. Previous experience, prejudice, and prior knowledge can also contribute to our estimates. The likelihood and prior terms can be combined through our friend, Bayes rule \cite{Bayes1763,Laplace1774}, to calculate the posterior probability of the state, i.e. the probability of the state given the observations

\begin{equation}
P(X|Y) = \frac{P(Y|X)P(X)}{P(Y)}.
\label{eq:BayesRule}
\end{equation}

This is the basis of the process of inference. Mathematically, we can assign a probability distribution to the state space of $X$. By applying Bayes rule, we are updating our belief about the values of $X$ using the information in $Y$.

Often the quantity in which we are interested, $X$, is changing over time, and it is desirable to estimate its value at each point in time given only the observations received so far. In this case, a hidden Markov model may be employed for the state evolution process, shown graphically in figure~\ref{fig:HMM}. This is the traditional filtering problem.

\begin{figure}[hbt]%
\input{tikz_HMM}
\caption{Graphical hidden Markov model.}%
\label{fig:HMM}%
\end{figure}

Our prior information about the state can now be derived from the estimate of its value at the previous time step with a forward prediction operation

\begin{equation}
P(X_t|Y_{1:t}) = \frac{\int P(Y_t|X_t)P(X_t|X_{t-1})P(X_{t-1}|Y_{1:t-1}) dX_{t-1}}{P(Y_t|Y_{1:t-1})}
\label{eq:SeqBayesRule}
\end{equation}

where the subscript indicates the time and ranges are notated by `$:$' in the MATLAB style. $P(X_t|Y_{1:t})$ is called the filtering distribution. Equation~\ref{eq:SeqBayesRule} describes the ideal Bayesian filter.