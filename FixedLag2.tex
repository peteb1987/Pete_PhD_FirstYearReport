Here we consider the mathematical framework for fixed-lag estimatation, as presented by \cite{Doucet2006} and \cite{Briers2006}.

As before, the target posterior distribution in which we are interested is the familiar $P(X_{1:t}|Y_{1:t})$. However, the proposal mechanism now becomes more complex, because we will be replacing states in an existing particle. We first propose a particle from which to take the state ``history'', that is $X_{1:t-L}$. However, if we take this from the particle distribution from the previous step we get more of the path than we need, because each particle is a set of states $X_{1:t-1}$. The final $L-1$ states will be replaced when by a new ``present'', $X'_{t-L+1:t}$, drawn from an importance distribution. The complete proposal is thus

\begin{equation}
\{X_{1:t-L}, X'_{t-L+1:t}\} \sim \int q(X_{1:t-1}|Y_{1:t-1}) q(X'_{t-L+1}|X_{1:t-1}, Y_{t-L+1:t}) dX_{t-L+1:t-1}
\label{eq:DumbFLProposal1}
\end{equation}

where $q(X_{1:t-1}|Y_{1:t-1})$ is a proposal distribution using the arbitrarily-weighted particles from $\hat{P}(X_{1:t-1}|Y_{1:t-1})$, in the auxiliary sampling sense. This integral is required for calculation of importance weights of acceptance probabilities, but in general it will be intractable. If we were to restrict our proposals to depend only on the history, i.e. use $q(X_{t-L+1:t}|X_{1:t-L}, Y_{t-L+1:t})$, then the proposal would become

\begin{equation}
\{X_{1:t-L}, X'_{t-L+1:t}\} \sim \hat{P}(X_{1:t-L}|Y_{1:t-1}) q(X'_{t-L+1}|X_{1:t-1}, Y_{t-L+1:t})
\label{eq:DumbFLProposal2}
\end{equation}

However, with this proposal, in order to evaluate importance weights or acceptance probabilities we still need to calculate

\begin{equation}
P(X_{1:t-L}|Y_{1:t-1}) = \frac{P(Y_{t-L+1:t-1}|X_{t-L}^{(m)}) P(X_{1:t-L}|Y_{1:t-L} }{P(Y_{t-L+1:t-1}|Y_{1:t-L})}
\label{eq:}
\end{equation}

The problematic term is:

\begin{equation}
P(Y_{t-L+1:t}|X^{(m)}_{t-L}) = \int P(Y_{t-L+1:t}|X^{(m)}_{t-L}, X_{t-L+1:t}) P(X_{t-L+1:t}|X^{(m)}_{t-L}) dX_{t-L+1:t}
\label{eq:}
\end{equation}

This is intractable. The solution to this problem proposed in \cite{Doucet2006} is to augment the dimension of the target distribution to include the discarded tracks. The new target is:

\begin{equation}
P(X_{1:t-L}, X'_{t-L+1:t}|Y_{1:t}) \rho(X_{t-L+1:t-1}|X_{1:t-L}, X'_{t-L+1:t})
\label{eq:}
\end{equation}

Once a particle approximation has been generated for this target the required posterior distribution may be obtained by marginalisation. As we are simply going to discard the old track sections $X_{t-L+1:t-1}$, the choice of $\rho(.)$ does not alter the distribution of the posterior. However, the variance of weights/acceptance probabilities may be affected.

With an expanded space for the target distribution there is no need to marginalise the previous states. The proposal distribution now becomes:

\begin{equation}
\{X_{1:t-1}, X'_{t-L+1:t}\} \sim q(X_{1:t-1}|Y_{1:t-1}) q(X'_{t-L+1:t}|X_{1:t-1}, Y_{t-L+1:t})
\label{eq:ExtendedFLProposal}
\end{equation}

Histories are proposed from the $t-1$ frame in the normal manner, see equation~\ref{eq:AuxiliarySamplingProposal}.

\cite{Doucet2006} show that the optimum choice (in the minimal importance weight variance sense) of artifical conditional distribution is given by 

\begin{equation}
\rho(X_{t-L+1:t-1}|X_{1:t-L}, X'_{t-L+1:t}) = P(X_{t-L+1:t-1}|X_{t-L}, Y_{t-L+1:t-1})
\label{eq:}
\end{equation}

with the current proposal distribution set to

\begin{equation}
q(X'_{t-L+1:t}|X_{1:t-s}, Y_{t-L+1:t}) = P(X'_{t-L+1:t}|X_{t-L}, Y_{t-L+1:t})
\label{eq:}
\end{equation}

Thus both artificial conditional and proposal distribution should equal to the conditional posterior. This is neither samplable nor calculable. However, it is intuitive to use the same approximation for each by making the artificial conditional equal to the proposal. The ratio of target to proposal probability is then given by:

\begin{multline}
\frac{ P(X_{1:t-L}, X'_{t-L+1:t}|Y_{1:t}) \rho(X_{t-L+1:t-1}|X_{1:t-L}, X'_{t-L+1:t}) }{ q(X_{1:t-1}|Y_{1:t-1}) q(X'_{t-L+1:t}|X_{1:t-1}, Y_{t-L+1:t}) } \\
= \frac{W_t}{V_t} \frac{ P(X_{1:t-L}, X'_{t-L+1:t}|Y_{1:t}) q(X_{t-L+1:t-1}|X_{1:t-L}, X'_{t-L+1:t-2}, Y_{t-L+1:t-1}) }{ P(X_{1:t-1}|Y_{1:t-1}) q(X'_{t-L+1:t}|X_{1:t-1}, Y_{t-L+1:t}) } \\
\propto \frac{W_t}{V_t} \frac{ P(Y_{t-L+1:t}|X'_{t-L+1:t}) P(X'_{t-L+1:t}|X_{t-L}) q(X_{t-L+1:t-1}|X_{1:t-L}, X'_{t-L+1:t-2}, Y_{t-L+1:t-1}) }{ P(Y_{t-L+1:t-1}|X_{t-L+1:t-1}) P(X_{t-L+1:t-1}|X_{t-L}) q(X'_{t-L+1:t}|X_{1:t-1}, Y_{t-L+1:t}) }
\label{eq:}
\end{multline}

ARGH! NO! THE PROPOSAL CAN ALSO DEPEND ON $Z$ in the MCMC CASE!!!





\subsection{Proposing older histories}
CONSIDER MOVING THIS TO THE MCMC IMPLEMENTATION SECTION
So far we have chosen to propose histories from the $t-1$ particle distribution. However, a suitable history could be also selected from the $t-s$ distribution, for $1 \le s \le L$. In a situation where a track has not been detected for several consecutive frames, taking a history from further in the past may make the proposal of a good candidate more probable.

PICTURE

The history proposal lag variable $s$ can most easily be handled by sampling it too.

\begin{equation}
\{X_{1:t-s}, X'_{t-L+1:t}, s\} \sim q(s) q(X_{1:t-s}|Y_{1:t-s}) q(X'_{t-L+1:t}|X_{1:t-s}, Y_{t-L+1:t})
\label{eq:ExtendedFLProposal}
\end{equation}

This is simply equivalent to enlarging the pool of particles from which we propose histories. $q(s)$ is simply a modification to the particle proposal weights.

SOMETHING ABOUT CONSERVATIVE RESAMPLING AND ITS EQUIVALENT AIM
