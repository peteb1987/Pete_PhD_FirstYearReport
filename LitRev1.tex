Two spheres of literature will be of interest here: the developments of models and algorithms for tracking, and the studies of the numerical inference schemes which underly our method. We begin with the latter.

\section{Particle filters}

Filtering is the operation of infering the state of an evolving latent random process given a set of imperfect observations up until the current time. In a Bayesian estimation scheme, we would like to calculate a posterior probability distribution for such a state at each point in time. The difficulty with such a procedure is that the complexity of the state distribution will tend to compound over time. In the particular case where both the state transition and observation processes are linear transformations with Gaussian noise, it is possible to derive an analytic expression for the state distribution with constant complexity, the fabulous Kalman filter (KF) of \cite{Kalman1960}. However, as many problems are oftern non-linear or non-Gaussian, and as no other such analytic and generally applicable cases have been discovered, \cite{Daum2005}, we must resort to numerical techniques.

The particle filter (PF) approximates a probability distribution using a set of discrete samples or ``particles'' drawn from it. This will be no trivial task - the distribution cannot be sampled directly, just as it cannot be expressed analytically. Instead, at each time step, the cloud of particles representing the previous distribution is propagated forwards to approximate the next, taking into account the latest observation. The most common PF algorithm in use is known as Sequential Importance Sampling with Resampling (SISR) (or some permutation of this initialism) - in fact the terms SISR and PF are often used interchangeably.

\subsection{Sequential importance sampling}

\subsubsection{Importance sampling}
The first modern implementation of the SISR algorithm by \cite{Gordon1993}. At the heart of the algorithm is an importance sampling step. Each particle from the old distribution is propagated forward by sampling from a proposal or `importance' distribution. The particles are then assigned an ``importance weight'' to account for the discrepancy between the proposal distribution and the desired posterior. A significant strength of the algorithm is that we need only be able to calculate the posterior probability up to a normalising constant. This may be easily accounted for by normalising the weights of the discrete sample set.

\subsubsection{Resampling}
Over time, the variance in these weights is liable to grow, as unlikely particles deviate completely from the correct state and are assigned a negligible weight. The eventual result is a degenerate sample with all the weight on one or a few particles and most contributing nothing to the approximation. The solution to this degeneracy problem is to introduce a resampling step before the importance sampling, in which particles are sampled from the particle approximation. Thus, heavily weighted particles are multiplied while low weight particles are discarded. Many algorithms for resampling exist, a summary of which is given by \cite{Doucet2009}. This resampling step introduces a degree of Monte Carlo error, and so \cite{Liu1995} introduce a measure of particle diversity to assess the degree of degeneracy. This measure, the effective sample size (ESS), is an estimate of the equivalent number of equal-weighted particles in the approximation, and is based on a calculation of the sample weight variance. Resampling is only used if the ESS falls below a chosen threshold.

\subsubsection{Importance distributions}
The filter proposed by \cite{Gordon1993}, known as the bootstrap filter, used the state transition density as the importance distribution, as did many other early implementations, \cite{Blake1998,Kitagawa1996}. However, this can result in poor performance if the observation process is informative. In other words, if we know that the observation is always very close to the state, then there is no point in proposing particles far away which will have a very low weight. It was suggested in, amongst others, \cite{Liu1995} that a better proposal could be constructed by considering the position of the new observation. In particular, the ``optimal'' (in the sense that it minimises the weight variance) proposal is the conditional distribution of the new state given the state history and the new observation (see \cite{Doucet2000a} for proof). This is rarely available analytically or samplable, so Gaussian approximations may be used \cite{Doucet2000a}.

\subsubsection{Auxiliary particle filtering}
In the description above, resampling was introduced as an additional procedure to rejeuvenate a degenerate particle distribution. However, it is possible to view it as a more integral part of the sampling procedure: At each step we construct a set of particles by proposing both a new state and a history for each. These histories are selected from the previous particle distribution. The resamping fulfils the role of a ``history proposal''. On a step with resampling we propose histories from the weighted particle distribution from the previous frame. On a step without resampling, we propose histories from the unweighted particle distribution, which can of course be done by simply keeping the same set of particles! In this new paradigm the obvious generalisation is to allow history proposals from the previous set of particles with arbitrary weights. A correction is made in the importance weight calculation to account for the proposal weights. This is the innovation of the auxliary particle filter introduced by \cite{Pitt1999}. The proposal weights may be assigned so as to favour particles which give rise to more likely states.

\subsubsection{Resample-move}
The objective when selecting the ``optimal'' proposal distribution or in choosing auxiliary proposal weights is to minimise the variance of the importance weights, and thus reduce the need for resampling. Another important particle filtering technique is the resample-move method of \cite{Gilks2001}, which aims instead to replenish the particle diversity after resampling. Rather than beginning the next IS step with multiple copies of the same particle, the current and previous states of each are adapted using one or more Metropolis-Hastings moves. Although this method improves the representation of the state distribution, it requires significant additional computation for each particle.



\subsection{Markov chain Monte Carlo}

\subsubsection{Metropolis-Hastings}
Markov chain Monte Carlo (MCMC) algorithms, like SISR, make use of a set of samples to approximate a probability distribution. Unlike SISR, MCMC is not specifically designed to address problems of sequential inference, although, as we shall see, they may be applied to these problems as an MCMC particle filter.

The basic MCMC method was proposed by \cite{Metropolis1953} and extended by \cite{Hastings1970}, after which two the Metropolis-Hastings (MH) algorithm is named. Rather than producing independent samples from a distribution, MH uses a Markov chain. A reversible, ergodic Markov chain has a stationary distribution. By careful design of the transition properties of the chain, we can make the stationary distribution equal to some desired target distribution. For our inference problems, this target will be the state posterior.

A MH move is made by sampling a value of the state from a proposal distribution. With some probability, the new value is accepted, otherwise the previous value is maintained. The acceptance probability is given by:

\begin{equation}
\alpha = \min \bigg ( 1,  \frac{ P(X_{\text{new}}) q(X_{\text{old}}|X_{\text{new}}) }{ P(X_{\text{old}}) q(X_{\text{new}}|X_{\text{old}}) }  \bigg )
\label{eq:}
\end{equation}

where $q(.|.)$ is the proposal distribution, and $P(.)$ the target.

The chain must be initialised at some value, and this is unlikely to be a sample from the target (if we could sample it, we would not need MCMC). Thus, there will be a period over which the sampler converges to the target distribution. This period is known as the ``burn in'' and the samples should be excluded from the particle approximation.

The choice of proposal does not affect the target distribution, although it will affect the rate at which the algorithm converges. If the proposal distribution has a low variance then successive samples will be close together, and it may take many moves to explore the whole distribution. If the variance is large, then the acceptance probability may be very low and again convergence may be slow.

A complete and excellent discussion of MCMC design considerations, convergence properties, etc. can be found in \cite{Gilks1996}, and will not be repeated here.

\subsubsection{Block sampling}
When MCMC algorithms are applied to multivariate distributions it is not necessary to change every variable in every move. If the state is divided into two blocks, $X_i$ and $X_{-i}$, and a move is proposed only on $X_i$, then the acceptance probability is now given by:

\begin{equation}
\alpha % = \bigg (  \frac{ P(X_{i,\text{new}}|X_{-i}) P(X_{-i}) q(X_{i,\text{old}}|X_{i,\text{new}}, X_{-i}) }{ P(X_{i,\text{old}}|X_{-i}) P(X_{-i}) q(X_{i,\text{new}}|X_{i,\text{old}}, X_{-i}) }  \bigg )
= \min \bigg ( 1,  \frac{ P(X_{i,\text{new}}|X_{-i}) q(X_{i,\text{old}}|X_{i,\text{new}}, X_{-i}) }{ P(X_{i,\text{old}}|X_{-i}) q(X_{i,\text{new}}|X_{i,\text{old}}, X_{-i}) }  \bigg )
\label{eq:}
\end{equation}

A special case of this scheme occurs when $q(X_{i,\text{new}}|X_{i,\text{old}}, X_{-i}) = P(X_{i,\text{new}}|X_{-i})$. This is the Gibbs sampler devised by \cite{Geman1984}. With this choice of proposal distribution, the acceptance probability cancels out to 1. Thus, all samples are accepted. The difficulty with this formulation is that the required conditional distribution may not be available, or may not be samplable.

MH moves which alter only a subset of the variables are commonly known as Metropolis-within-Gibbs (MwG) moves.

\subsubsection{Reversible jump Markov chain Monte Carlo}
MCMC methods are not restricted to distributions of fixed dimension. The Reversible Jump MCMC (RJMCMC) method devised by \cite{Green1995} allows MH moves to jump between state spaces with different dimensionality. This is especially useful when trying to choose between models while simultaneously estimating parameters, or for estimating the order or number of components in a model, for example in a Gaussian mixture or autoregressive model.

RJMCMC moves may have very low acceptance probabilities. When the sampler jump from one state space to another, there may be no obvious function for generating likely values in the new state. For such cases, \cite{Al-Awadhi2004} propose using a system of compound moves, where a model change move is followed by a number of normal MH moves designed to seek out a likely set of state values. A single acceptance probability is then calculated for the set of moves. 

\subsubsection{Sequential Markov chain Monte Carlo}
The interest in MCMC for this project lies is in their application to sequential inference problems. Such `MCMC particle filters' were first proposed by \cite{Khan2005}, although this first implementation suffered from computation issues, and \cite{Golightly2006}, and have recently been applied to a range of problems including target tracking, \cite{Septier2009}, group tracking, e.g. \cite{Carmi2009}, tracking of a correlated stock prices, \cite{Pang2011} and music transcription, \cite{Bunch2010}.

MENTION POLSON'S PRACTICAL FILTER SOMEWHERE - try to understand it first.

The MCMC particle filter runs a Markov chain for each frame which targets the posterior state distribution at that time. The particles are thus unweighted and are generated in series rather than in parallel, with each particle depending on the last. The state space of the Markov chain consists of the entire history of the hidden variables as well as the latest values. Two types of MH move are thus required: History-moves involve proposing a new history from the particles of the previous particle distribution. Current-moves involve a proposal of the latest variables given the history and data. The two types of move are analogous to the resampling and importance sampling steps of the SISR algorithm respectively.

MCMC particle filters have a significant advantage over their SISR counterparts for complex, high dimensional problems. As noted in \cite{Pang2008}, we can use MwG moves, changing only a few variables at once, to explore the posterior state distribution. In contrast, with an SISR particle filter, we must propose a new value for every variable at once. In some cases this enables the MCMC particle filter to give a better approximation of the state posterior, as demonstrated in \cite{Pang2011}.



\subsection{Coping with dimensionality}
It is a commonly noted problem that the efficacy of particle filters falls as the number of state dimensions increases. This is a problem for both SISR and MCMC filters, and indeed for any Monte Carlo approximation. There is a strong intuition for this phenomenon - it will clearly take more particles to adequately sample a square than it would a line, and more still for a cube. An analysis of this effect, including numerical studies, was conducted in \cite{Daum2003}.

The severity of the ``curse of dimensionality'' will depend on the correlation between the state variables, and the ability to exploit such correlations by the design of effective proposal distributions. Methods to improve performance of MCMC algorithms include Hybrid Monte Carlo (HMC) \cite{Duane1987}, which introduces ``momentum'' variables to the state in order to build better proposals and the Riemann manifold methods of \cite{Girolami2011}.



\subsection{Marginalised particle filters}
Monte Carlo methods are effective for tackling nonlinear, non-Gaussian problems when analytic methods cannot be found. However, they are computationally expensive and so should be avoided when an analytic solution does exist. In some problems, the state can be partitioned into two parts, one which behaves in a linear-Gaussian way conditional on the other. In such a case a particle filter can be used for inference of the nonlinear part, and a Kalman filter for the linear-Gaussian part. Such a strategy reduces the variance of the state estimates, in accordance with the Rao-Blackwell theorem. See, for example, \cite{Casella1996}. The only change required is that likelihood calculation for the particle filter will now require the linear-Gaussian part of the state to be marginalised. Such schemes may be used with both SISR and MCMC particle filters and are known as \emph{marginalised} or \emph{Rao-Blackwellised} particle filters.


