\subsection{Importance distributions}

The dimensionality of a fixed lag particle filter will generally be large. Consider a problem where each target has a 4-dimensional kinematic state (position and velocity in $x$ and $y$) and an observation-association index. This gives us five dimensions per target per time step. If we use a lag window with length $L=5$ and 5 targets we will have a 125 dimensional state. If we used a basic, bootstrap approach to such a problem, the chances of even a single particle following the correct path are negligible. Instead we must exploit the strong correlations between states arising from the structure of the problem. This is achieved by designing better proposal distributions. We aim to approximate the `optimal' importance distribution of equation~\ref{eq:OptimalImportanceDist}. The same approximation will also be useful as the artificial conditional distribution required for extending the state-space for the fixed lag particle filters. We first factorise the proposal over the targets

\begin{equation}
q(X_{t-L+1:t}, \Lambda_{t-L+1:t}|X_{t-L}, Y_{t-L+1:t}) = \prod_{j=1}^{K_t} q(x_{j,t-L+1:t}, \lambda_{j,t-L+1:t}|\lambda_{1:j-1,t-L+1:t}, x_{j,t-L}, Y_{t-L+1:t}).
\end{equation}

Thus the proposal may be conducted sequentially by target. The proposal for each target is then further decomposed as

\begin{IEEEeqnarray}{rCl}
\IEEEeqnarraymulticol{3}{l}{q(x_{j,t-L+1:t}, \lambda_{j,t-L+1:t}|\lambda_{1:j-1,t-L+1:t}, x_{j,t-L}, Y_{t-L+1:t})} \nonumber \\
\qquad \qquad \qquad \qquad & = & q(\lambda_{j,t-L+1:t}|\lambda_{1:j-1,t-L+1:t}, x_{j,t-L}, Y_{t-L+1:t}) \nonumber \\
 & = & q(x_{j,t-L+1:t}|x_{j,t-L}, \lambda_{j,t-L+1:t}, Y_{t-L+1:t}).
\end{IEEEeqnarray}

Hence we can sample first the association variables then the state variables. Each of these terms can be further factorised over time, as we shall see. If we were to replace each of the factors with its corresponding posterior distribution then this factorisation would recreate the optimal importance distribution.



\subsubsection{Association proposals}

We first consider a proposal distribution for a single target. The proposal for the association variables is a discrete distribution over the possible observations with which the target could be associated. The `optimal' form of the proposal is

\begin{IEEEeqnarray}{rCl}
\IEEEeqnarraymulticol{3}{l}{q(\lambda_{t-L+1:t}|x_{t-L}, Y_{t-L+1:t})} \nonumber \\
\qquad \qquad \qquad \qquad & \propto & P(Y_{t-L+1:t}|\lambda_{t-L+1:t},x_{t-L}) P(\lambda_{t-L+1:t}|x_{t-L}) \nonumber \\
 & = & \prod_{\substack{k=1:L\\l=t-L+k}} P(Y_{l}|Y_{l+1:t} \lambda_{l:t}, x_{t-L}) P(\lambda_{l}) \nonumber \\
 & = & \prod_{\substack{k=1:L\\l=t-L+k}} \int P(Y_{l}|x_{l}, \lambda_{l}) P(x_{l}|Y_{l+1:t}, \lambda_{l+1:t}, x_{t-L}) dx_{l} P(\lambda_{l}).
\label{eq:GeneralAssocProp}
\end{IEEEeqnarray}

This suggests a convenient sequential sampling procedure, starting with $\lambda_t$ and working backwards in time. The normalisation constant is not known, but as this is discrete distribution we can enforce normalisation by dividing by the sum.

First we consider a factor from this expression with $\lambda_{l}=0$, i.e. a proposal that in a particular frame the target is not detected. In this case, the observation density is independent of the state of the target, and we have

\begin{equation}
P(Y_{l}|Y_{l+1:t} \lambda_{l:t}, x_{t-L}) P(\lambda_{l}) = V^{-1} \mu_C (1-P_D).
\end{equation}

When the target is detected, the factors of the proposal distribution of equation~\ref{eq:GeneralAssocProp} may be calculated analytically for the linear-Gaussian case. For other cases, the EKF approximations may be used. As this is only a proposal distribution, such an approximation will not affect the distribution of the particles generated. The state distribution term over $x_{l}$ is problematic, requiring $k$ integrals to calculate. Although this is will be analytic with Gaussian dynamics, its complexity may become unmanageable. This term represents the probability of the state given the set of observations associated with this target at later times. We can render this calculation more manageable by replacing the whole set of future observations with just one.

\begin{equation}
P(x_k|Y_{k+1:t}, \lambda_{k+1:t}, x_{t-L}) \approx P(x_k|Y_{k+d}, \lambda_{k+d}, x_{t-L})
\end{equation}

For cases with low observation noise, where an observation gives us significant information about the state of the target, this substitution will have little effect. Later observations cannot add much additional information. Again, as this is a proposal distribution, such a substitution will not affect the validity of the resulting particle distribution.

In general we will use $d=1$, as the closest observation in time will give us the most information about $x_{l}$. However, if the target is not detected at time $l+1$, then we can increase $d$ to pick out the next detection of the target.

Using this approximation, for $\lambda_{t-L+k} \ne 0$, we have

\begin{equation}
P(Y_{l}|Y_{l+1:t} \lambda_{l:t}, x_{t-L}) P(\lambda_{l}) \propto P_D \mathcal{N}(y_{l}^{(\lambda_{l})}|m_{l}, S_{l})
\end{equation}

where usually

\begin{equation} S_{l} = [ I - R^{-1} C_{l} \Sigma_{l} C_{l} R^{-1} ]^{-1} \end{equation}
\begin{equation} m_{l} = S R^{-1} C_{l} \Sigma_{l} [ (A^d)^T C_{l+d}^T R_d^{-1} y_{l+d}^{\lambda_{l+d}} + Q_k^{-1} A^k x_{t-L} ] \end{equation}
\begin{equation} \Sigma_{l} = [ C_{l}^T R^{-1} C_{l} + (A^d)^T C_{l+d}^T R_d^{-1} C_{l+d} A^d + Q_k^{-1}]^{-1} \end{equation}
\begin{equation} Q_d = \sum_{l=0}^{d-1} {A^l Q (A^l)^T} \end{equation}
\begin{equation} R_d = R + C_{l+d} Q_d (C_{l+d})^T \end{equation}

We will need a different expression for the case when $l=t$, because no future associations have yet been proposed. Similarly, if $l<t$ but the future associations have all been proposed as missed detections, then there are no future observations to guide us, whatever choice of $d$ we use. In these cases we have

\begin{equation} S_{l} = R_d \end{equation}
\begin{equation} m_{l} = C_{l} A^k x_{t-L} .\end{equation}

For full derivations, see Appendix~\ref{chap:App_AssocProposals}.

Thus, combining everything

\begin{equation}
q(\lambda_{t-L+1:t}|x_{t-L}, Y_{t-L+1:t}) \propto \prod_{\substack{k=1:L\\l=t-L+k}} \begin{cases}
P_D \mathcal{N}(y_{l}^{(\lambda_{l})}|m_{l}, S_{l}) & \lambda_{l}=0 \\
(1-P_D) \mu_C V^{-1} & \lambda_{l} \ne 0 \end{cases}.
\end{equation}

This scheme can be extended to multiple targets by simply sampling targets sequentially. The only modification required is that the proposal probability is set to zero if an observation has already been associated with another target.

Realistically, we cannot calculate a proposal weight for all possible associations. If we have a large observation area and many false alarms this would require a vast number of calculations of weights which would mostly be negligible. Instead we use a gating procedure, considering only targets within a certain Mahalanobis squared distance of the proposal mean. i.e. those for which

\begin{equation}
(y_{l}^{(\lambda_{l})} - m_{l})^T S_{l}^{-1} (y_{l}^{(\lambda_{l})} - m_{l}) < M_{\text{thresh}}.
\end{equation}

This Mahalanobis squared distance will be chi-square distributed with $d$ degrees of freedom, where $d$ is the dimension of $y_{l}$. To demonstrate this, consider the definition of a chi-square variable as (time indices omitted)

\begin{equation}
\chi_d^2 = \sum_{i=1}^d z_i^2 = z^T z
\end{equation}

where $z_i$ are i.i.d standard normal variables. Now apply a transformation

\begin{equation}
y = m + U \Lambda^{\frac{1}{2}} z
\end{equation}

where $U$ and $\Lambda$ are derived from the eigenvalue decomposition

\begin{equation}
S = U \Lambda U^t .
\end{equation}

Thus

\begin{equation}
\chi_d^2 = (y-m) S^{-1} (y-m)
\end{equation}

Hence $M_{\text{thresh}}$ can be chosen from the tabulated cumulative chi-square distribution to ensure a given probability of catching the correct observation in the gate. Choosing $M_{\text{thresh}}=16$ (4 standard deviations) gives a probability of just 0.03\% of excluding the correct observation with 2 degrees of freedom. Note however that when a nonlinear model is in use this will rise significantly due to the linear approximation. A generous choice of $M_{\text{thresh}}=25$ (5 standard deviations) is thus used for nonlinear cases.



\subsubsection{State proposals}

Once the associations are fixed, the states can be proposed. When the state space model is linear-Gaussian, we can propose directly from the `optimal' importance distribution for the states using the forward-filtering-backward-sampling algorithm of \cite{Chib1996}, as suggested in \cite{Doucet2006}. For nonlinear models, we can use EKF approximations. Once again, we factorise the proposal

\begin{IEEEeqnarray}{rCl}
\IEEEeqnarraymulticol{3}{l}{q(x_{t-L+1:t}|x_{t-L}, \lambda_{t-L+1:t}, Y_{t-L+1:t})} \nonumber \\
\qquad & = & P(x_{t-L+1:t}|x_{t-L}, \lambda_{t-L+1:t}, Y_{t-L+1:t}) \nonumber \\
 & = & P(x_t|\lambda_{t-L+1:t}, Y_{t-L+1:t}, x_{t-L}) \prod_{k=t-L+1}^{t-1} P(x_k|\lambda_{t-L+1:k}, Y_{t-L+1:k}, x_{t-L}, x_{k+1})
\end{IEEEeqnarray}

where

\begin{equation}
P(x_k|\lambda_{t-L+1:k}, Y_{t-L+1:k}, x_{t-L}, x_{k+1}) \propto P(x_{k+1}|x_k) P(x_k|\lambda_{t-L+1:k}, Y_{t-L+1:k}, x_{t-L}).
\end{equation}

The distributions $P(x_k|\lambda_{t-L+1:k}, Y_{t-L+1:k}, x_{t-L})$ are given by a Kalman filter, and are Gaussian with mean $\mu_k$ and covariance $\Sigma_k$. Thus the complete state proposal is given by

\begin{equation}
q(x_{t-L+1:t}|x_{t-L}, \lambda_{t-L+1:t}, Y_{t-L+1:t}) = \mathcal{N}(x_t|\mu_t, \Sigma_t) \prod_{k=t-L+1}^{t-1} \mathcal{N}(x_k|m_k, S_k)
\end{equation}

where

\begin{equation}S_k = [ A^T Q^{-1} A + \Sigma^{-1} ]^{-1}\end{equation}
\begin{equation}m_k = S_k [ A^T Q^{-1} x_{k+1} + \Sigma^{-1} \mu_k ] .\end{equation}

This method is equivalent to proposing states from a Kalman smoother estimate over the window.
